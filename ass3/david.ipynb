{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3719e224",
   "metadata": {
    "papermill": {
     "duration": 0.007856,
     "end_time": "2024-05-04T03:16:21.078297",
     "exception": false,
     "start_time": "2024-05-04T03:16:21.070441",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sampling large Datasets\n",
    "In data processing, a great deal of computing involves analysing large amounts of text mixed with numerical data.  This is what Spark is particularly suited for. Sampling is an essential pre-processing for machine leanring for proof of concept\n",
    "\n",
    "## Recbole dataset\n",
    "Recbole is a powerful recommendation system traning and evaluation platform. It has many built-in datasets(https://recbole.io/dataset_list.html), some of which is too large to process on a single computer. I will use spark to preprocess it to shrink its size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e74f01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T03:16:21.095582Z",
     "iopub.status.busy": "2024-05-04T03:16:21.094859Z",
     "iopub.status.idle": "2024-05-04T03:17:19.000063Z",
     "shell.execute_reply": "2024-05-04T03:17:18.998603Z"
    },
    "papermill": {
     "duration": 57.917518,
     "end_time": "2024-05-04T03:17:19.003214",
     "exception": false,
     "start_time": "2024-05-04T03:16:21.085696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\r\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\r\n",
      "Building wheels for collected packages: pyspark\r\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=d51218b3f65615c08ebd283832cfe7afa1d6ca62b18a478e7e1718c7e497719c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\r\n",
      "Successfully built pyspark\r\n",
      "Installing collected packages: pyspark\r\n",
      "Successfully installed pyspark-3.5.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e29302",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T03:17:19.042697Z",
     "iopub.status.busy": "2024-05-04T03:17:19.041960Z",
     "iopub.status.idle": "2024-05-04T03:17:36.792065Z",
     "shell.execute_reply": "2024-05-04T03:17:36.790472Z"
    },
    "papermill": {
     "duration": 17.773003,
     "end_time": "2024-05-04T03:17:36.795150",
     "exception": false,
     "start_time": "2024-05-04T03:17:19.022147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'url.yaml': No such file or directory\r\n",
      "--2024-05-04 03:17:21--  https://raw.githubusercontent.com/RUCAIBox/RecBole/master/recbole/properties/dataset/url.yaml\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 16548 (16K) [text/plain]\r\n",
      "Saving to: 'url.yaml'\r\n",
      "\r\n",
      "url.yaml            100%[===================>]  16.16K  --.-KB/s    in 0.003s  \r\n",
      "\r\n",
      "2024-05-04 03:17:21 (4.92 MB/s) - 'url.yaml' saved [16548/16548]\r\n",
      "\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (6.0.1)\r\n",
      "adult : https://recbole.s3-accelerate.amazonaws.com/ProcessedDatasets/Adult/adult.zip\n",
      "alibaba-ifashion : https://recbole.s3-accelerate.amazonaws.com/ProcessedDatasets/Alibaba-iFashion/Alibaba-iFashion.zip\n",
      "aliec : https://recbole.s3-accelerate.amazonaws.com/ProcessedDatasets/AliEC/AliEC.zip\n",
      "amazon-apps-for-android : https://recbole.s3-accelerate.amazonaws.com/ProcessedDatasets/Amazon_ratings/Amazon_Apps_for_Android.zip\n",
      "amazon-automotive : https://recbole.s3-accelerate.amazonaws.com/ProcessedDatasets/Amazon_ratings/Amazon_Automotive.zip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!rm url.yaml\n",
    "!wget https://raw.githubusercontent.com/RUCAIBox/RecBole/master/recbole/properties/dataset/url.yaml\n",
    "!pip install pyyaml\n",
    "\n",
    "import yaml\n",
    "\n",
    "# Specify the path to the YAML file\n",
    "file_path = \"url.yaml\"\n",
    "\n",
    "# Open the file and load the YAML contents\n",
    "with open(file_path, \"r\") as file:\n",
    "    dataset_urls = yaml.safe_load(file)\n",
    "   \n",
    "# only print the first 5 lines\n",
    "for key in list(dataset_urls.keys())[:5]:\n",
    "    print(key, \":\", dataset_urls[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772716f4",
   "metadata": {
    "papermill": {
     "duration": 0.019899,
     "end_time": "2024-05-04T03:17:36.834017",
     "exception": false,
     "start_time": "2024-05-04T03:17:36.814118",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Set the datasets to donwload and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54c826d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T03:17:36.873675Z",
     "iopub.status.busy": "2024-05-04T03:17:36.873149Z",
     "iopub.status.idle": "2024-05-04T03:17:36.880566Z",
     "shell.execute_reply": "2024-05-04T03:17:36.879469Z"
    },
    "papermill": {
     "duration": 0.030137,
     "end_time": "2024-05-04T03:17:36.882948",
     "exception": false,
     "start_time": "2024-05-04T03:17:36.852811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets_to_download = ['amazon-books', 'amazon-movies-tv']\n",
    "\n",
    "import os\n",
    "# Path to the folder where the zip file will be extracted\n",
    "input_folder_path = \"input\"\n",
    "\n",
    "# Create input folder if it doesn't exist\n",
    "if not os.path.exists(input_folder_path):\n",
    "    os.makedirs(input_folder_path)\n",
    "    \n",
    "# Path to the folder where processed file will be saved\n",
    "output_folder_path = \"output\"\n",
    "\n",
    "# Create out folder if it doesn't exist\n",
    "if not os.path.exists(output_folder_path):\n",
    "    os.makedirs(output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6faf1808",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T03:17:36.924432Z",
     "iopub.status.busy": "2024-05-04T03:17:36.923972Z",
     "iopub.status.idle": "2024-05-04T03:18:09.089755Z",
     "shell.execute_reply": "2024-05-04T03:18:09.088350Z"
    },
    "papermill": {
     "duration": 32.190475,
     "end_time": "2024-05-04T03:18:09.092932",
     "exception": false,
     "start_time": "2024-05-04T03:17:36.902457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (2.31.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests) (2024.2.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "def download_upzip(url, dataset_name):\n",
    "    # Download the zip file\n",
    "    response = requests.get(url)\n",
    "    zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "    # Extract the zip file to the specified folder of dataset_name\n",
    "    folder_path = os.path.join(input_folder_path, dataset_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    zip_file.extractall(folder_path)\n",
    "\n",
    "    #TODO: if extracted file is a directory, move all files to the parent directory\n",
    "    # for root, dirs, files in os.walk(folder_path):\n",
    "    #     for file in files:\n",
    "    #         os.rename(os.path.join(root, file), os.path.join(folder_path, file))\n",
    "    #     for dir in dirs:\n",
    "    #         os.rmdir(os.path.join(root, dir))\n",
    "\n",
    "    # Close the zip file\n",
    "    zip_file.close()\n",
    "\n",
    "#  download all dataset from datasets_to_download\n",
    "for dataset in datasets_to_download:\n",
    "    download_upzip(dataset_urls[dataset], dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3addb81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T03:18:09.134529Z",
     "iopub.status.busy": "2024-05-04T03:18:09.134052Z",
     "iopub.status.idle": "2024-05-04T03:18:15.465487Z",
     "shell.execute_reply": "2024-05-04T03:18:15.464290Z"
    },
    "papermill": {
     "duration": 6.355837,
     "end_time": "2024-05-04T03:18:15.468348",
     "exception": false,
     "start_time": "2024-05-04T03:18:09.112511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/04 03:18:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Building Spark Session\n",
    "spark = (SparkSession.builder.appName(\"RecBole Sampling\")\n",
    "            .config(\"spark.driver.memory\", \"24G\")\n",
    "            .config(\"spark.executor.memory\", \"6G\")\n",
    "            .config(\"spark.executor.cores\",\"4\")\n",
    "            .getOrCreate())\n",
    "#spark.sparkContext.setLogLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "865be2d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T03:18:15.512067Z",
     "iopub.status.busy": "2024-05-04T03:18:15.510805Z",
     "iopub.status.idle": "2024-05-04T03:18:15.522937Z",
     "shell.execute_reply": "2024-05-04T03:18:15.521611Z"
    },
    "papermill": {
     "duration": 0.037414,
     "end_time": "2024-05-04T03:18:15.525998",
     "exception": false,
     "start_time": "2024-05-04T03:18:15.488584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84d87ebc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T03:18:15.567784Z",
     "iopub.status.busy": "2024-05-04T03:18:15.567338Z",
     "iopub.status.idle": "2024-05-04T03:20:29.147049Z",
     "shell.execute_reply": "2024-05-04T03:20:29.145713Z"
    },
    "papermill": {
     "duration": 133.605365,
     "end_time": "2024-05-04T03:20:29.151324",
     "exception": false,
     "start_time": "2024-05-04T03:18:15.545959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: amazon-books, File: Amazon_Books.inter\n",
      "+--------------+-------------+------------+---------------+\n",
      "| user_id:token|item_id:token|rating:float|timestamp:float|\n",
      "+--------------+-------------+------------+---------------+\n",
      "| AH2L9G3DQHHAJ|   0000000116|         4.0|     1019865600|\n",
      "|A2IIIDRK3PRRZY|   0000000116|         1.0|     1395619200|\n",
      "|A1TADCM7YWPQ8M|   0000000868|         4.0|     1031702400|\n",
      "| AWGH7V0BDOJKB|   0000013714|         4.0|     1383177600|\n",
      "|A3UTQPQPM4TQO0|   0000013714|         5.0|     1374883200|\n",
      "+--------------+-------------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of Amazon_Books.inter: 22507155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of disintict user_id:token: 8026324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of disintict item_id:token: 2330066\n",
      "Number of non-null values in each column:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+------------+---------------+\n",
      "|user_id:token|item_id:token|rating:float|timestamp:float|\n",
      "+-------------+-------------+------------+---------------+\n",
      "|     22507155|     22507155|    22507155|       22507155|\n",
      "+-------------+-------------+------------+---------------+\n",
      "\n",
      "Dataset: amazon-books, File: Amazon_Books.item\n",
      "+-------------+----------------+----------------+--------------------+--------------------+-----------+-----------+\n",
      "|item_id:token|sales_type:token|sales_rank:float|categories:token_seq|         title:token|price:float|brand:token|\n",
      "+-------------+----------------+----------------+--------------------+--------------------+-----------+-----------+\n",
      "|   0001048791|           Books|       6334800.0|             'Books'|The Crucible: Per...|       NULL|       NULL|\n",
      "|   0001048775|           Books|      13243226.0|             'Books'|Measure for Measu...|       NULL|       NULL|\n",
      "|   0001048236|           Books|       8973864.0|             'Books'|The Sherlock Holm...|       9.26|       NULL|\n",
      "|   0000401048|           Books|       6448843.0|             'Books'|The rogue of publ...|       NULL|       NULL|\n",
      "|   0001019880|           Books|       9589258.0|             'Books'|Classic Soul Winn...|       5.39|       NULL|\n",
      "+-------------+----------------+----------------+--------------------+--------------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of Amazon_Books.item: 2370604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of disintict item_id:token: 2370604\n",
      "Number of non-null values in each column:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+----------------+--------------------+-----------+-----------+-----------+\n",
      "|item_id:token|sales_type:token|sales_rank:float|categories:token_seq|title:token|price:float|brand:token|\n",
      "+-------------+----------------+----------------+--------------------+-----------+-----------+-----------+\n",
      "|      2370604|         1891174|         1891163|             2370585|    1938767|    1679399|        106|\n",
      "+-------------+----------------+----------------+--------------------+-----------+-----------+-----------+\n",
      "\n",
      "Dataset: amazon-movies-tv, File: Amazon_Movies_and_TV.item\n",
      "+-------------+--------------------+--------------------+-----------+----------------+----------------+-----------+\n",
      "|item_id:token|categories:token_seq|         title:token|price:float|sales_type:token|sales_rank:float|brand:token|\n",
      "+-------------+--------------------+--------------------+-----------+----------------+----------------+-----------+\n",
      "|   0000143561|'Movies', 'Movies...|Everyday Italian ...|      12.99|     Movies & TV|        376041.0|       NULL|\n",
      "|   0000589012|'Movies', 'Movies...|Why Don't They Ju...|      15.95|     Movies & TV|       1084845.0|       NULL|\n",
      "|   0000695009|'Movies', 'Movies...|Understanding Sei...|       NULL|     Movies & TV|       1022732.0|       NULL|\n",
      "|   000107461X|'Movies', 'Movies...|Live in Houston [...|       NULL|     Movies & TV|        954116.0|       NULL|\n",
      "|   0000143529|'Movies', 'Movies...|My Fair Pastry (G...|      19.99|     Movies & TV|        463562.0|       NULL|\n",
      "+-------------+--------------------+--------------------+-----------+----------------+----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "num of Amazon_Movies_and_TV.item: 208328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of disintict item_id:token: 208326\n",
      "Number of non-null values in each column:\n",
      "+-------------+--------------------+-----------+-----------+----------------+----------------+-----------+\n",
      "|item_id:token|categories:token_seq|title:token|price:float|sales_type:token|sales_rank:float|brand:token|\n",
      "+-------------+--------------------+-----------+-----------+----------------+----------------+-----------+\n",
      "|       208328|              208325|     107676|     155624|          204904|          204902|      12314|\n",
      "+-------------+--------------------+-----------+-----------+----------------+----------------+-----------+\n",
      "\n",
      "Dataset: amazon-movies-tv, File: Amazon_Movies_and_TV.inter\n",
      "+--------------+-------------+------------+---------------+\n",
      "| user_id:token|item_id:token|rating:float|timestamp:float|\n",
      "+--------------+-------------+------------+---------------+\n",
      "|A3R5OBKS7OM2IR|   0000143502|         5.0|     1358380800|\n",
      "|A3R5OBKS7OM2IR|   0000143529|         5.0|     1380672000|\n",
      "| AH3QC2PC1VTGP|   0000143561|         2.0|     1216252800|\n",
      "|A3LKP6WPMP9UKX|   0000143588|         5.0|     1236902400|\n",
      "| AVIY68KEPQ5ZD|   0000143588|         5.0|     1232236800|\n",
      "+--------------+-------------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of Amazon_Movies_and_TV.inter: 4607047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of disintict user_id:token: 2088620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of disintict item_id:token: 200941\n",
      "Number of non-null values in each column:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 65:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+------------+---------------+\n",
      "|user_id:token|item_id:token|rating:float|timestamp:float|\n",
      "+-------------+-------------+------------+---------------+\n",
      "|      4607047|      4607047|     4607047|        4607047|\n",
      "+-------------+-------------+------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# read from file into dataframe\n",
    "dfs = {}\n",
    "for dataset in datasets_to_download:\n",
    "    dataset_path = os.path.join(input_folder_path, dataset)\n",
    "    dfs[dataset] = {}\n",
    "    for file in os.listdir(dataset_path):\n",
    "        file_path = os.path.join(dataset_path, file)\n",
    "        df = spark.read.option(\"delimiter\",'\\t').option(\"header\", True).csv(file_path)\n",
    "        dfs[dataset][file] = df\n",
    "        print(f\"Dataset: {dataset}, File: {file}\")\n",
    "        df.show(5)\n",
    "        \n",
    "        print(f'num of {file}:',df.count())\n",
    "\n",
    "        # check the uniqueness of key, we assume key name is ending with _id bofore :token i.e. item_id:token\n",
    "        # find the header ending with _id:token\n",
    "        key_columns = [col for col in df.columns if col.endswith('_id:token')]\n",
    "        for key_column in key_columns:\n",
    "            print(f\"Number of disintict {key_column}:\", df.select(key_column).distinct().count())\n",
    "            \n",
    "\n",
    "        # check the completeness of each column\n",
    "        print(\"Number of non-null values in each column:\")\n",
    "        df.select([count(when(col(c).isNotNull() , c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cfe75f",
   "metadata": {
    "papermill": {
     "duration": 0.02637,
     "end_time": "2024-05-04T03:20:29.207702",
     "exception": false,
     "start_time": "2024-05-04T03:20:29.181332",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "025fdd21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T03:20:29.264105Z",
     "iopub.status.busy": "2024-05-04T03:20:29.263698Z",
     "iopub.status.idle": "2024-05-04T03:20:29.270672Z",
     "shell.execute_reply": "2024-05-04T03:20:29.269387Z"
    },
    "papermill": {
     "duration": 0.038203,
     "end_time": "2024-05-04T03:20:29.273609",
     "exception": false,
     "start_time": "2024-05-04T03:20:29.235406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inter_map = {}\n",
    "# analyze the sparse of the dataset\n",
    "for dataset in datasets_to_download:\n",
    "    dataset_path = os.path.join(input_folder_path, dataset)\n",
    "    for file in os.listdir(dataset_path):\n",
    "        if file.endswith('.inter'):\n",
    "            inter_map[dataset] = file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84089ab7",
   "metadata": {
    "papermill": {
     "duration": 0.025237,
     "end_time": "2024-05-04T03:20:29.324966",
     "exception": false,
     "start_time": "2024-05-04T03:20:29.299729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### filter out inactive user/items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24970f8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T03:20:29.379101Z",
     "iopub.status.busy": "2024-05-04T03:20:29.378687Z",
     "iopub.status.idle": "2024-05-04T03:26:23.037012Z",
     "shell.execute_reply": "2024-05-04T03:26:23.035205Z"
    },
    "papermill": {
     "duration": 353.689428,
     "end_time": "2024-05-04T03:26:23.040258",
     "exception": false,
     "start_time": "2024-05-04T03:20:29.350830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Dataset: amazon-books\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of iteractions: 22507155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of user_id: 8026324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of item_id: 2330066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+------------+---------------+----------+----------+\n",
      "|item_id:token| user_id:token|rating:float|timestamp:float|count_user|count_item|\n",
      "+-------------+--------------+------------+---------------+----------+----------+\n",
      "|   0000095699|A1QHY69FQH9F5R|         3.0|     1254700800|         4|         1|\n",
      "|   0001048775|A2M4YJ7ANBGYKD|         2.0|     1264550400|        21|         1|\n",
      "|   0001064487|A3GFXEFR8FDX6P|         5.0|     1309046400|         1|         4|\n",
      "|   0001064487|A17K364R0ETIJJ|         5.0|     1355961600|         2|         4|\n",
      "|   0001064487|A1V9HZP9ONKV78|         5.0|     1367280000|        11|         4|\n",
      "+-------------+--------------+------------+---------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered num of iteractions: 6789807\n",
      "-----------------------------------\n",
      "Dataset: amazon-movies-tv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of iteractions: 4607047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of user_id: 2088620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of item_id: 200941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------------+---------------+----------+----------+\n",
      "|item_id:token|       user_id:token|rating:float|timestamp:float|count_user|count_item|\n",
      "+-------------+--------------------+------------+---------------+----------+----------+\n",
      "|   B00AQN09G6|A0358075SYJ9W13JC9RE|         5.0|     1403308800|         1|       159|\n",
      "|   B000E6EK42|A04004323EMIP0JQX...|         5.0|     1403568000|         1|       203|\n",
      "|   B0009S4IO2|      A1001IQ9OI5H47|         5.0|     1123113600|         2|        16|\n",
      "|   B003UESJH4|      A100NGGXRQF0AQ|         5.0|     1304035200|         6|      1209|\n",
      "|   B00HEPDGKA|      A100OFVFM8WLFE|         3.0|     1396396800|         1|       839|\n",
      "+-------------+--------------------+------------+---------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered num of iteractions: 1204688\n"
     ]
    }
   ],
   "source": [
    "user_inter_threshold = 10\n",
    "item_inter_threshold = 10\n",
    "\n",
    "# filter out the user and item with less than threshold interactions\n",
    "for dataset in datasets_to_download:\n",
    "    print('-----------------------------------')\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    inter_df = dfs[dataset][inter_map[dataset]]\n",
    "    \n",
    "    print(f'num of iteractions:',inter_df.count())\n",
    "\n",
    "    # print(f'num of {inter_map[dataset]}:',inter_df.count())\n",
    "    print(f'num of user_id:',inter_df.select('user_id:token').distinct().count())\n",
    "    print(f'num of item_id:',inter_df.select('item_id:token').distinct().count())\n",
    "    # count the number of interactions for each user and item and rename the count column\n",
    "    user_count_df = inter_df.groupBy('user_id:token').count().withColumnRenamed('count','count_user')\n",
    "    item_count_df = inter_df.groupBy('item_id:token').count().withColumnRenamed('count','count_item')\n",
    "\n",
    "    # append the count of user and item to the original df\n",
    "    inter_df = inter_df.join(user_count_df, on='user_id:token', how='inner')\n",
    "    inter_df = inter_df.join(item_count_df, on='item_id:token', how='inner')\n",
    "    inter_df.show(5)\n",
    "    \n",
    "    # filter out the user and item with less than threshold interactions\n",
    "    inter_df = inter_df.filter((col('count_user') >= user_inter_threshold) & (col('count_item') >= item_inter_threshold))\n",
    "    \n",
    "    print(f'filtered num of iteractions:',inter_df.count())\n",
    "    \n",
    "    # release the memory of dfs[dataset][inter_map[dataset]]\n",
    "    dfs[dataset][inter_map[dataset]] = inter_df.drop('count_user','count_item')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a557fc",
   "metadata": {
    "papermill": {
     "duration": 0.03796,
     "end_time": "2024-05-04T03:26:23.117987",
     "exception": false,
     "start_time": "2024-05-04T03:26:23.080027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Output overlaped users between datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1df0857c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T03:26:23.196673Z",
     "iopub.status.busy": "2024-05-04T03:26:23.196212Z",
     "iopub.status.idle": "2024-05-04T03:26:23.203132Z",
     "shell.execute_reply": "2024-05-04T03:26:23.201616Z"
    },
    "papermill": {
     "duration": 0.050082,
     "end_time": "2024-05-04T03:26:23.205653",
     "exception": false,
     "start_time": "2024-05-04T03:26:23.155571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# folder list of output folders\n",
    "output_folder_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cf89076",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T03:26:23.393463Z",
     "iopub.status.busy": "2024-05-04T03:26:23.393041Z",
     "iopub.status.idle": "2024-05-04T03:53:05.937400Z",
     "shell.execute_reply": "2024-05-04T03:53:05.935465Z"
    },
    "papermill": {
     "duration": 1602.697501,
     "end_time": "2024-05-04T03:53:05.941379",
     "exception": false,
     "start_time": "2024-05-04T03:26:23.243878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common users between amazon-books and amazon-movies-tv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "| user_id:token|\n",
      "+--------------+\n",
      "|A100UD67AHFODS|\n",
      "|A100WFKYVRPVX7|\n",
      "|A1018BDG082EVM|\n",
      "|A101OMG474Q26I|\n",
      "|A102K2AH06SY4L|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of common_users: 15690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 313:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of interactino of common users in amazon-books: 818364\n",
      "num of related items in the interaction: 218393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "density of amazon-books inetraction : 3.799025416546306e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+------------+---------------+\n",
      "| user_id:token|item_id:token|rating:float|timestamp:float|\n",
      "+--------------+-------------+------------+---------------+\n",
      "|A100UD67AHFODS|   0143121340|         5.0|     1353974400|\n",
      "|A100UD67AHFODS|   0307352145|         5.0|     1351814400|\n",
      "|A100UD67AHFODS|   0544217624|         5.0|     1390953600|\n",
      "|A100UD67AHFODS|   0553245767|         5.0|     1352073600|\n",
      "|A100UD67AHFODS|   0615818455|         5.0|     1393459200|\n",
      "+--------------+-------------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of interactino of common users in amazon-movies-tv: 604613\n",
      "num of related items in the interaction: 48575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "density of amazon-movies-tv inetraction : 4.1018926864298005e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+------------+---------------+\n",
      "| user_id:token|item_id:token|rating:float|timestamp:float|\n",
      "+--------------+-------------+------------+---------------+\n",
      "|A100UD67AHFODS|   B004HW7JH4|         5.0|     1350950400|\n",
      "|A100UD67AHFODS|   6304179103|         5.0|     1351814400|\n",
      "|A100UD67AHFODS|   B000H5U5EE|         5.0|     1351814400|\n",
      "|A100UD67AHFODS|   B0056Q0V98|         5.0|     1351900800|\n",
      "|A100UD67AHFODS|   B000E0WJUK|         5.0|     1151884800|\n",
      "+--------------+-------------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "base_dataset = datasets_to_download[0]\n",
    "# find the common users between base_dataset and other datasets\n",
    "for j in range(1,len(datasets_to_download)):\n",
    "        dataset1 = base_dataset\n",
    "        dataset2 = datasets_to_download[j]\n",
    "        inter1 = dfs[dataset1][inter_map[dataset1]]\n",
    "        inter2 = dfs[dataset2][inter_map[dataset2]]\n",
    "        inter1.createOrReplaceTempView(\"inter1\")\n",
    "        inter2.createOrReplaceTempView(\"inter2\")\n",
    "\n",
    "        print(f\"Common users between {dataset1} and {dataset2}\")    \n",
    "        # get the distinct users and then intersect\n",
    "        inter1_dist = inter1.select('user_id:token').distinct()\n",
    "        # inter1_dist.show(5)\n",
    "        common_users = inter1_dist.join(inter2, inter1_dist['user_id:token'] == inter2['user_id:token'],'leftsemi')\n",
    "        common_users.show(5)\n",
    "\n",
    "        print(f'num of common_users:',common_users.count())\n",
    "        # print the items count of each inter of common users\n",
    "        inter1_com_user = inter1.join(common_users, 'user_id:token')\n",
    "        inter2_com_user = inter2.join(common_users, 'user_id:token')\n",
    "        # statictics of inter 1\n",
    "        inter1_com_user_count = inter1_com_user.count()\n",
    "        inter1_com_item_count = inter1_com_user.select('item_id:token').distinct().count()\n",
    "        print(f'num of interactino of common users in {dataset1}:',inter1_com_user_count)\n",
    "        print(f'num of related items in the interaction:',inter1_com_item_count)\n",
    "        print(f'density of {dataset1} inetraction :',inter1.count()/inter1_com_user_count/inter1_com_item_count)\n",
    "\n",
    "        # save filtered datasets to file\n",
    "        inter1_out_path = os.path.join(output_folder_path, f\"{dataset1}_{dataset2}\")\n",
    "        inter1_com_user.show(5)\n",
    "        inter1_com_user.repartition(1).write.option(\"header\", \"true\").csv(inter1_out_path, mode='overwrite', sep='\\t')\n",
    "        output_folder_list.append(inter1_out_path)\n",
    "        # output_folder_map[dataset1] = inter1_out_path\n",
    "        \n",
    "        # statictics of inter 2\n",
    "        inter2_com_user_count = inter2_com_user.count()\n",
    "        inter2_com_item_count = inter2_com_user.select('item_id:token').distinct().count()\n",
    "        print(f'num of interactino of common users in {dataset2}:',inter2_com_user_count)\n",
    "        print(f'num of related items in the interaction:',inter2_com_item_count)\n",
    "        print(f'density of {dataset2} inetraction :',inter2.count()/inter2_com_user_count/inter2_com_item_count)\n",
    "\n",
    "        # save filtered datasets to file\n",
    "        inter2_out_path = os.path.join(output_folder_path, f\"{dataset2}_{dataset1}\")\n",
    "        inter2_com_user.show(5) \n",
    "        inter2_com_user.repartition(1).write.option(\"header\", \"true\").csv(inter2_out_path, mode='overwrite', sep='\\t')\n",
    "        output_folder_list.append(inter2_out_path)\n",
    "        # output_folder_map[dataset2] = inter2_out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95550e16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T03:53:06.155391Z",
     "iopub.status.busy": "2024-05-04T03:53:06.154875Z",
     "iopub.status.idle": "2024-05-04T04:24:07.215782Z",
     "shell.execute_reply": "2024-05-04T04:24:07.214471Z"
    },
    "papermill": {
     "duration": 1861.169147,
     "end_time": "2024-05-04T04:24:07.221043",
     "exception": false,
     "start_time": "2024-05-04T03:53:06.051896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common users among all datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "| user_id:token|\n",
      "+--------------+\n",
      "|A1J482FVR1LR6P|\n",
      "|A17SPEC8D1SX85|\n",
      "|A1PCEZZ6LE72WK|\n",
      "+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of common_users after merge with amazon-books: 293885\n",
      "Common users among all datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "| user_id:token|\n",
      "+--------------+\n",
      "|A140XH16IKR4B0|\n",
      "|A17SPEC8D1SX85|\n",
      "|A1ABI2GH9C5FG0|\n",
      "+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of common_users after merge with amazon-movies-tv: 15690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "| user_id:token|\n",
      "+--------------+\n",
      "|A17SPEC8D1SX85|\n",
      "|A1W2JCN01CTT1V|\n",
      "|A33352GGJ0UVRF|\n",
      "+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of interactino of common users in amazon-books: 818364\n",
      "num of amazon-books : 218393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 895:===========>                                             (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "density of amazon-books inetraction : 3.799025416546306e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+------------+---------------+\n",
      "| user_id:token|item_id:token|rating:float|timestamp:float|\n",
      "+--------------+-------------+------------+---------------+\n",
      "|A100UD67AHFODS|   0143121340|         5.0|     1353974400|\n",
      "|A100UD67AHFODS|   0307352145|         5.0|     1351814400|\n",
      "|A100UD67AHFODS|   0544217624|         5.0|     1390953600|\n",
      "|A100UD67AHFODS|   0553245767|         5.0|     1352073600|\n",
      "|A100UD67AHFODS|   0615818455|         5.0|     1393459200|\n",
      "+--------------+-------------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of interactino of common users in amazon-movies-tv: 604613\n",
      "num of amazon-movies-tv : 48575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "density of amazon-movies-tv inetraction : 4.1018926864298005e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+------------+---------------+\n",
      "| user_id:token|item_id:token|rating:float|timestamp:float|\n",
      "+--------------+-------------+------------+---------------+\n",
      "|A100UD67AHFODS|   B004HW7JH4|         5.0|     1350950400|\n",
      "|A100UD67AHFODS|   6304179103|         5.0|     1351814400|\n",
      "|A100UD67AHFODS|   B000H5U5EE|         5.0|     1351814400|\n",
      "|A100UD67AHFODS|   B0056Q0V98|         5.0|     1351900800|\n",
      "|A100UD67AHFODS|   B000E0WJUK|         5.0|     1151884800|\n",
      "+--------------+-------------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# find the common users among all downloaded datasets\n",
    "for dataset in datasets_to_download:\n",
    "    inter = dfs[dataset][inter_map[dataset]]\n",
    "    inter.createOrReplaceTempView(\"inter\")\n",
    "\n",
    "    print(f\"Common users among all datasets\")    \n",
    "    # get the distinct users and then intersect\n",
    "    inter_dist = inter.select('user_id:token').distinct()\n",
    "    inter_dist.show(3)\n",
    "    if dataset == datasets_to_download[0]:\n",
    "        common_users = inter_dist\n",
    "    else:\n",
    "        common_users = common_users.join(inter_dist, 'user_id:token','inner')\n",
    "    print(f'num of common_users after merge with {dataset}:',common_users.count())\n",
    "\n",
    "common_users.show(3)\n",
    "\n",
    "# export inter of common users to file\n",
    "for dataset in datasets_to_download:\n",
    "    inter = dfs[dataset][inter_map[dataset]]\n",
    "    inter.createOrReplaceTempView(\"inter\")\n",
    "    inter_com_user = inter.join(common_users, 'user_id:token')\n",
    "    inter_com_user_count = inter_com_user.count()\n",
    "    inter_com_item_count = inter_com_user.select('item_id:token').distinct().count()\n",
    "    print(f'num of interactino of common users in {dataset}:',inter_com_user_count)\n",
    "    print(f'num of {dataset} :',inter_com_item_count)\n",
    "    print(f'density of {dataset} inetraction :',inter.count()/inter_com_user_count/inter_com_item_count)\n",
    "\n",
    "    # save filtered datasets to file\n",
    "    inter_out_path = os.path.join(output_folder_path, f\"{dataset}_common\")\n",
    "    inter_com_user.show(5)\n",
    "    inter_com_user.repartition(1).write.option(\"header\", \"true\").csv(inter_out_path, mode='overwrite', sep='\\t')\n",
    "    output_folder_list.append(inter_out_path)\n",
    "    # output_folder_map[dataset] = inter_out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50d4e54d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T04:24:07.543363Z",
     "iopub.status.busy": "2024-05-04T04:24:07.542910Z",
     "iopub.status.idle": "2024-05-04T04:24:20.233665Z",
     "shell.execute_reply": "2024-05-04T04:24:20.232134Z"
    },
    "papermill": {
     "duration": 12.855363,
     "end_time": "2024-05-04T04:24:20.236817",
     "exception": false,
     "start_time": "2024-05-04T04:24:07.381454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy from input/amazon-books/Amazon_Books.item to output/amazon-books_amazon-movies-tv for amazon-books \n",
      "copy from input/amazon-movies-tv/Amazon_Movies_and_TV.item to output/amazon-movies-tv_amazon-books for amazon-movies-tv \n",
      "copy from input/amazon-books/Amazon_Books.item to output/amazon-books_common for amazon-books \n",
      "copy from input/amazon-movies-tv/Amazon_Movies_and_TV.item to output/amazon-movies-tv_common for amazon-movies-tv \n"
     ]
    }
   ],
   "source": [
    "dataset_itemfile_map = {}\n",
    "def get_itemfile_path(dataset):\n",
    "    dataset_path = os.path.join(input_folder_path, dataset)\n",
    "    for file in os.listdir(dataset_path):\n",
    "        if file.endswith('.item'):\n",
    "            return os.path.join(dataset_path, file)\n",
    "    return None\n",
    "\n",
    "for ouptput_folder in output_folder_list:\n",
    "    # strip the dataset from the first part of folder\n",
    "    dataset = os.path.basename(ouptput_folder).split('_')[0]\n",
    "    # copy .item file from correonding input folder to output folder\n",
    "    itemfile_path = get_itemfile_path(dataset)\n",
    "    if itemfile_path:\n",
    "        print(f\"copy from {itemfile_path} to {ouptput_folder} for {dataset} \")\n",
    "        out_path = os.path.join(ouptput_folder, f\"{dataset}.item\")\n",
    "        !cp $itemfile_path $out_path\n",
    "    else:\n",
    "        print(f\"item file not found for {dataset}\")\n",
    "\n",
    "    for file in os.listdir(ouptput_folder):\n",
    "        # rename exported cvs as .inter\n",
    "        if file.endswith('.csv'):\n",
    "            # rename file to {folder}.inter\n",
    "            file_path = os.path.join(ouptput_folder, file)\n",
    "            out_path = os.path.join(ouptput_folder, f\"{dataset}.inter\")\n",
    "            !mv $file_path $out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a80af",
   "metadata": {
    "papermill": {
     "duration": 0.23572,
     "end_time": "2024-05-04T04:24:20.633256",
     "exception": false,
     "start_time": "2024-05-04T04:24:20.397536",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Analyze Chronicle Characteristics\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c61299",
   "metadata": {
    "papermill": {
     "duration": 0.158661,
     "end_time": "2024-05-04T04:24:20.950239",
     "exception": false,
     "start_time": "2024-05-04T04:24:20.791578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sampling\n",
    "Stratified sampling based on hotness(interaction rate) of items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c30a8e",
   "metadata": {
    "papermill": {
     "duration": 0.162806,
     "end_time": "2024-05-04T04:24:21.272061",
     "exception": false,
     "start_time": "2024-05-04T04:24:21.109255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## release all the resources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12b51546",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T04:24:21.591569Z",
     "iopub.status.busy": "2024-05-04T04:24:21.591091Z",
     "iopub.status.idle": "2024-05-04T04:24:22.231865Z",
     "shell.execute_reply": "2024-05-04T04:24:22.230235Z"
    },
    "papermill": {
     "duration": 0.804044,
     "end_time": "2024-05-04T04:24:22.235030",
     "exception": false,
     "start_time": "2024-05-04T04:24:21.430986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# unpersist the dfs\n",
    "for dataset in datasets_to_download:\n",
    "    for key in dfs[dataset]:\n",
    "        dfs[dataset][key].unpersist()\n",
    "        \n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe44935e",
   "metadata": {
    "papermill": {
     "duration": 0.160112,
     "end_time": "2024-05-04T04:24:22.560926",
     "exception": false,
     "start_time": "2024-05-04T04:24:22.400814",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sammary\n",
    "Spark is a powerful and efficient tool to handle sample on large scale of data. \n",
    "* flexible and powerful functionality\n",
    "* runs super fast even on my laptop\n",
    "* easy to apply to similar datasets(Amazon have dataset of different categories), I only focused on one categoy this time. "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4088.007553,
   "end_time": "2024-05-04T04:24:25.359528",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-04T03:16:17.351975",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
