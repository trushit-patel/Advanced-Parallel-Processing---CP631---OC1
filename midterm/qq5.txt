I believe that in this scenario, Amdahl's law is relevant as it help us to understand the level of speed optimization that we have achieved after parallelizing the code. This law is also relevant because it can help to understand the affects of both parallelizable and non-parallelizable parts of the program on the overall performance when running on multiple processors.

Below is an estimate of the speed up of the above program -:

Amdahl's law -> S(N)=1/((1âˆ’P)+(P/N))

Here S(N) is the speedup with N processors, P is the proportion of program that can be parallelized and N is the number of processors.

Now, in our case, the total execution time is 1000s. Time spent in executing function f 200s, time spent on executing function g 800s and the number of processes is 1024. Here function f is non-parallelizable and function g is parallelizable. To calculate the proportion P we will do -:

P = parallelizable time/total time
P = 800/100
P = 0.8


Now applying Amdahl's law we get-:


S(1024) = 1/((1-0.8)+(0.8/1024))

S(1024) = 1/((0.2)+(0.00078125))

S(1024) = 1/(0.20078125)

S(1024) ~ 4.98

4.98 is the approximate speed up. This means that when we run the program on 1024 processes, the speedup achieved is almost 5 times when compared to running it on a single process.