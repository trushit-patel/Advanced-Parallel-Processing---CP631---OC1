The total time required by the parallel algorithm to complete can be divided into two main components: computation time and communication time.

1. Computation Time:
   Each process is responsible for finding the maximum values along each row in its local portion of the matrix. For each row, this involves comparing the elements in N/p columns. Since each process handles N rows, the total number of comparisons per process is N * (N / p).

   Let tmaxcomp be the time needed to compare two numbers. Therefore, the total computation time per process is:
   
   Tcomp = N * (N / p) * tmaxcomp = (N^2 / p) * tmaxcomp

2. Communication Time:
   After computing the local maximum values, each process participates in an MPI_Reduce operation to find the global maximum values along each row. This operation involves communicating N elements (one for each row) among p processes.

   Let tcomm be the time needed to communicate one element. The total communication time is proportional to the number of elements and the number of processes:

   Tcomm = N * p * tcomm

3. Total Time:
   The total time required by the parallel algorithm is the sum of the computation time and the communication time. Therefore, the total time Ttotal is given by:

   Ttotal = Tcomp + Tcomm
          = (N^2 / p) * tmaxcomp + N * p * tcomm

In conclusion, the expression that estimates the total amount of time required by the parallel algorithm is:

   Ttotal = (N^2 / p) * tmaxcomp + N * p * tcomm